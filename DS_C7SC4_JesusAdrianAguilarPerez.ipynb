{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPDyQyADKTcnwJ9yeWTrlWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jadrianp29/testreko/blob/main/DS_C7SC4_JesusAdrianAguilarPerez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo que distingue entre Perros y Gatos\n",
        "\n",
        "Objetivos:\n",
        "\n",
        "Diseñar modelos de redes neuronales profundas (deep learning), enfocándose en la clasificación de imágenes de acuerdo a categorías preestablecidas para resolver problemas con relevancia social, permitiendo generar valor en los diversos sectores.\n",
        "Crear modelos de datos Deep Neural Networks (DNN) utilizando PyTorch basado en Python; seleccionando el modelo adecuado y analizando la exactitud del modelo, para cumplir lo mejor posible con los requerimientos de la tarea requerida."
      ],
      "metadata": {
        "id": "z4-HtT8tOgpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar librerias"
      ],
      "metadata": {
        "id": "qdYyf-xN3V1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rmQHiuBbKRVc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "33PBrFfe6Xev"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando carpeta"
      ],
      "metadata": {
        "id": "UVLPE5P14hkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckqiGbMB4hHU",
        "outputId": "c314ca51-114c-4c0a-8c59-a110dc740a22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos Database asignamos un tamaño de 128*128 y hacemos data augmentation\n"
      ],
      "metadata": {
        "id": "ts3jGyk94K2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "BwyVx6gg4UvG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos directorio"
      ],
      "metadata": {
        "id": "WyVn7ORB5NPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = datasets.ImageFolder(\"/content/drive/MyDrive/Tensorflow/DS_C7_SC4/catsvsdogs\", transform=transform)"
      ],
      "metadata": {
        "id": "1hgfbacs4aM9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea conjunto de entrenamiento y validacion"
      ],
      "metadata": {
        "id": "9vBnCycx--a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(DIR))\n",
        "val_size = len(DIR) - train_size\n",
        "train_dataset, val_dataset = random_split(DIR, [train_size, val_size])\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "TCHmKTRu_Ei4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcion para Crear minibatches"
      ],
      "metadata": {
        "id": "In7flaBh3Xzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "for img, label in train_dataset:\n",
        "    x_train.append(img.unsqueeze(0))\n",
        "    y_train.append(label)\n",
        "\n",
        "x_train = torch.cat(x_train)\n",
        "y_train = torch.tensor(y_train)"
      ],
      "metadata": {
        "id": "vHudmaqWAIND"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_minibatches(x,y, mb_size):\n",
        "  total_data = x.shape[0]\n",
        "  idxs = np.arange(total_data)\n",
        "  np.random.shuffle(idxs)\n",
        "  x = x[idxs]\n",
        "  y = y[idxs]\n",
        "  return [(x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size)]"
      ],
      "metadata": {
        "id": "Kg8IA5ATa-QG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minibatches = create_minibatches(x_train, y_train, mb_size=16)"
      ],
      "metadata": {
        "id": "rjw5PrjkAHC7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcion para entrenar el modelo, Se utiliza Binary Cross Entropy (BCE) ya que solo son 2 clases"
      ],
      "metadata": {
        "id": "T2j4qS7iOagh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, mb_size, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for xb, yb in create_minibatches(x_train, y_train, mb_size):\n",
        "      xb = xb.to(device=device, dtype=torch.float32)\n",
        "      model.train()\n",
        "      scores = model(xb)\n",
        "      yb = yb.to(device=device, dtype=torch.long)\n",
        "      cost = F.cross_entropy(scores, yb)\n",
        "      optimizer.zero_grad()\n",
        "      cost.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 20 == 0:\n",
        "      print(f\"Epoch: {epoch}, Cost: {cost.item()},accuracy:{accuracy(model,val_loader)}\")\n"
      ],
      "metadata": {
        "id": "I2PCZ7bJLFwT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creacion de Red neuronal usando optimizador adam"
      ],
      "metadata": {
        "id": "Hk1IgfHr3lX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = nn.Sequential(\n",
        "    nn.Conv2d(3, 20, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    nn.Conv2d(20, 40, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout2d(0.5),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(40 * 32 * 32, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 2)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "MnmVT3SwPd5W"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_final=model1.to(device=device)\n",
        "adam=torch.optim.Adam(modelo_final.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "p73yA-yO9kfy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcion para ver el accuracy de nuestro modelo"
      ],
      "metadata": {
        "id": "hs_k7ySK3pRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model,loader):\n",
        "  num_correct = 0\n",
        "  num_total = 0\n",
        "  model.eval()\n",
        "  model= model.to(device=device)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for(xb, yb) in loader:\n",
        "       xb = xb.to(device=device, dtype=torch.float32)\n",
        "       yb = yb.to(device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "       scores = model(xb)\n",
        "       _, pred= scores.max(dim=1)\n",
        "       num_correct += (pred == yb.squeeze()).sum()\n",
        "       num_total += pred.size(0)\n",
        "     return float(num_correct) / num_total"
      ],
      "metadata": {
        "id": "gpFBnyvVzC1d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(modelo_final,adam,200,200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WszEFaqI96N3",
        "outputId": "807ecc02-c716-49d3-fbfe-512914a9cba6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Cost: 0.648689329624176,accuracy:0.7215007215007215\n",
            "Epoch: 20, Cost: 0.5330218076705933,accuracy:0.7344877344877345\n",
            "Epoch: 40, Cost: 0.17983798682689667,accuracy:0.6897546897546898\n",
            "Epoch: 60, Cost: 0.04365674778819084,accuracy:0.7215007215007215\n",
            "Epoch: 80, Cost: 0.031046763062477112,accuracy:0.7027417027417028\n",
            "Epoch: 100, Cost: 0.017089765518903732,accuracy:0.7056277056277056\n",
            "Epoch: 120, Cost: 0.01745196431875229,accuracy:0.7186147186147186\n",
            "Epoch: 140, Cost: 0.010031916201114655,accuracy:0.70995670995671\n",
            "Epoch: 160, Cost: 0.010847355239093304,accuracy:0.7070707070707071\n",
            "Epoch: 180, Cost: 0.0025334935635328293,accuracy:0.7272727272727273\n"
          ]
        }
      ]
    }
  ]
}